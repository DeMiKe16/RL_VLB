{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#@title Instalamos gym\n",
    "!pip install swig\n",
    "!pip install \"gymnasium[box2d]\"\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "#!pip install \"gymnasium[toy-text]\"\n",
    "!pip install Box2D\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Importamos librerias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import Box2D\n",
    "from typing import Dict, Any\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Importamos el lago helado\n",
    "name = 'Taxi-v3'\n",
    "\n",
    "# Creamos el entorno Taxi-v3\n",
    "env = gym.make(name, render_mode=\"ansi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMonteCarloOnPolicy:\n",
    "    def __init__(self, env: gym.Env, hyperparameters: Dict = None):\n",
    "        \"\"\"\n",
    "        Inicializa el agente Monte Carlo On-Policy\n",
    "        \n",
    "        Args:\n",
    "            env: Entorno de Gymnasium\n",
    "            hyperparameters: Diccionario con los hiperparámetros (opcional)\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        \n",
    "        # Hiperparámetros con valores por defecto\n",
    "        if hyperparameters is None:\n",
    "            hyperparameters = {}\n",
    "        \n",
    "        self.gamma = hyperparameters.get('gamma', 0.99)\n",
    "        self.epsilon = hyperparameters.get('epsilon', 1.0)\n",
    "        self.epsilon_min = hyperparameters.get('epsilon_min', 0.01)\n",
    "        self.epsilon_decay = hyperparameters.get('epsilon_decay', 0.995)\n",
    "        \n",
    "        # Tablas para el aprendizaje\n",
    "        self.q_table = {}  # Valores Q: Q(s,a)\n",
    "        self.returns = {}  # Almacena retornos para cada par estado-acción\n",
    "        \n",
    "        # Buffer para almacenar el episodio actual\n",
    "        self.episode_buffer = []\n",
    "        \n",
    "        # Métricas de aprendizaje\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "    def get_action(self, state: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Política epsilon-soft para selección de acciones\n",
    "        \n",
    "        Args:\n",
    "            state: Estado actual del entorno\n",
    "        \n",
    "        Returns:\n",
    "            action: Acción seleccionada\n",
    "        \"\"\"\n",
    "        state_key = tuple(state) if isinstance(state, np.ndarray) else state\n",
    "        \n",
    "        # Inicializar estado si no existe\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = {\n",
    "                action: 0.0 for action in range(self.env.action_space.n)\n",
    "            }\n",
    "        \n",
    "        # Política epsilon-soft\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return max(self.q_table[state_key].items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def update(self, state: Any, action: Any, next_state: Any, \n",
    "               reward: float, terminated: bool, truncated: bool, info: Dict):\n",
    "        \"\"\"\n",
    "        Actualiza el agente con la nueva transición\n",
    "        \n",
    "        Args:\n",
    "            state: Estado actual\n",
    "            action: Acción tomada\n",
    "            next_state: Siguiente estado\n",
    "            reward: Recompensa obtenida\n",
    "            terminated: Si el episodio terminó naturalmente\n",
    "            truncated: Si el episodio fue truncado\n",
    "            info: Información adicional del entorno\n",
    "        \"\"\"\n",
    "        # Almacenar la transición en el buffer del episodio\n",
    "        self.episode_buffer.append((state, action, reward))\n",
    "        \n",
    "        # Actualizar métricas del episodio actual\n",
    "        self.current_episode_reward += reward\n",
    "        self.current_episode_length += 1\n",
    "        \n",
    "        # Si el episodio terminó, procesar el episodio completo\n",
    "        if terminated or truncated:\n",
    "            self._process_episode()\n",
    "            \n",
    "            # Guardar métricas del episodio\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "            \n",
    "            # Resetear métricas y buffer para el siguiente episodio\n",
    "            self.current_episode_reward = 0\n",
    "            self.current_episode_length = 0\n",
    "            self.episode_buffer = []\n",
    "            \n",
    "            # Actualizar epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def _process_episode(self):\n",
    "        \"\"\"\n",
    "        Procesa el episodio completo usando Monte Carlo first-visit\n",
    "        \"\"\"\n",
    "        # Calcular retornos para cada paso\n",
    "        G = 0\n",
    "        returns = []\n",
    "        \n",
    "        # Calcular retornos desde el final del episodio\n",
    "        for _, _, reward in reversed(self.episode_buffer):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Actualizar valores Q para cada paso (first-visit)\n",
    "        states_actions_seen = set()\n",
    "        \n",
    "        for idx, ((state, action, _), G) in enumerate(zip(self.episode_buffer, returns)):\n",
    "            state_key = tuple(state) if isinstance(state, np.ndarray) else state\n",
    "            sa_pair = (state_key, action)\n",
    "            \n",
    "            # Solo actualizar en la primera visita\n",
    "            if sa_pair not in states_actions_seen:\n",
    "                states_actions_seen.add(sa_pair)\n",
    "                \n",
    "                # Inicializar si es necesario\n",
    "                if state_key not in self.q_table:\n",
    "                    self.q_table[state_key] = {\n",
    "                        a: 0.0 for a in range(self.env.action_space.n)\n",
    "                    }\n",
    "                if sa_pair not in self.returns:\n",
    "                    self.returns[sa_pair] = []\n",
    "                \n",
    "                # Actualizar retornos y valor Q\n",
    "                self.returns[sa_pair].append(G)\n",
    "                self.q_table[state_key][action] = np.mean(self.returns[sa_pair])\n",
    "    \n",
    "    def stats(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Retorna estadísticas del aprendizaje\n",
    "        \n",
    "        Returns:\n",
    "            Dict con estadísticas relevantes\n",
    "        \"\"\"\n",
    "        if not self.episode_rewards:\n",
    "            return {\n",
    "                'mean_reward': 0,\n",
    "                'mean_length': 0,\n",
    "                'current_epsilon': self.epsilon,\n",
    "                'n_episodes': 0,\n",
    "                'q_table_size': len(self.q_table)\n",
    "            }\n",
    "            \n",
    "        return {\n",
    "            'mean_reward': np.mean(self.episode_rewards[-100:]),  # Media últimos 100 episodios\n",
    "            'mean_length': np.mean(self.episode_lengths[-100:]),  # Media últimos 100 episodios\n",
    "            'current_epsilon': self.epsilon,\n",
    "            'n_episodes': len(self.episode_rewards),\n",
    "            'q_table_size': len(self.q_table)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 54/1000 [00:00<00:03, 266.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio 0:\n",
      "Reward media (últimos 100): -857.00\n",
      "Longitud media (últimos 100): 200.00\n",
      "Epsilon actual: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 169/1000 [00:00<00:02, 357.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio 100:\n",
      "Reward media (últimos 100): -741.36\n",
      "Longitud media (últimos 100): 193.35\n",
      "Epsilon actual: 0.603\n",
      "\n",
      "Episodio 200:\n",
      "Reward media (últimos 100): -647.47\n",
      "Longitud media (últimos 100): 168.61\n",
      "Epsilon actual: 0.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 412/1000 [00:01<00:01, 487.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio 300:\n",
      "Reward media (últimos 100): -482.06\n",
      "Longitud media (últimos 100): 166.37\n",
      "Epsilon actual: 0.221\n",
      "\n",
      "Episodio 400:\n",
      "Reward media (últimos 100): -335.42\n",
      "Longitud media (últimos 100): 157.58\n",
      "Epsilon actual: 0.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 575/1000 [00:01<00:00, 525.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio 500:\n",
      "Reward media (últimos 100): -282.32\n",
      "Longitud media (últimos 100): 167.78\n",
      "Epsilon actual: 0.081\n",
      "\n",
      "Episodio 600:\n",
      "Reward media (últimos 100): -246.79\n",
      "Longitud media (últimos 100): 142.36\n",
      "Epsilon actual: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 747/1000 [00:01<00:00, 526.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio 700:\n",
      "Reward media (últimos 100): -261.86\n",
      "Longitud media (últimos 100): 160.13\n",
      "Epsilon actual: 0.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 902/1000 [00:01<00:00, 466.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio 800:\n",
      "Reward media (últimos 100): -211.94\n",
      "Longitud media (últimos 100): 150.56\n",
      "Epsilon actual: 0.018\n",
      "\n",
      "Episodio 900:\n",
      "Reward media (últimos 100): -177.62\n",
      "Longitud media (últimos 100): 169.16\n",
      "Epsilon actual: 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 451.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "n_episodes = 1000\n",
    "agent = AgentMonteCarloOnPolicy(env)\n",
    "\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Actualizar el agente\n",
    "        agent.update(obs, action, next_obs, reward, terminated, truncated, info)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    \n",
    "    # Obtener estadísticas\n",
    "    if episode % 100 == 0:\n",
    "        stats = agent.stats()\n",
    "        print(f\"\\nEpisodio {episode}:\")\n",
    "        print(f\"Reward media (últimos 100): {stats['mean_reward']:.2f}\")\n",
    "        print(f\"Longitud media (últimos 100): {stats['mean_length']:.2f}\")\n",
    "        print(f\"Epsilon actual: {stats['current_epsilon']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
