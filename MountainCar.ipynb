{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jESCWJeTcsUb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "import random\n",
        "from gymnasium import ObservationWrapper\n",
        "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip  # Import from correct locationimport torch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cacnHp_fdiDI"
      },
      "outputs": [],
      "source": [
        "semilla = 42\n",
        "np.random.seed(semilla)\n",
        "random.seed(semilla)\n",
        "torch.manual_seed(semilla)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(semilla)\n",
        "    torch.cuda.manual_seed_all(semilla)  # Para múltiples GPUs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "MaUJU3SIRZZC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6fNK8s5hghR"
      },
      "source": [
        "# Metodos Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pa2KUsbKhgHJ"
      },
      "outputs": [],
      "source": [
        "def pi_star_from_Q(env, Q):  # Add seed parameter\n",
        "    frames = []\n",
        "    done = False\n",
        "    pi_star = {}\n",
        "    obs, info = env.reset(seed = semilla)\n",
        "    active_features = env.last_active_features\n",
        "    img = env.render()\n",
        "    frames.append(img)\n",
        "    actions = \"\"\n",
        "    while not done:\n",
        "        q_values = [np.sum(Q[active_features, a]) for a in range(env.action_space.n)]\n",
        "        action = np.argmax(q_values)\n",
        "        pi_star[tuple(obs)] = action\n",
        "        actions += f\"{action}, \"\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        img = env.render()\n",
        "        frames.append(img)\n",
        "        done = terminated or truncated\n",
        "        active_features = env.last_active_features\n",
        "    return pi_star, actions, frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "94_Q-rr5hgM0"
      },
      "outputs": [],
      "source": [
        "def plot_training_results(rewards):\n",
        "    # Calculamos la media móvil para suavizar la gráfica\n",
        "    window_size = 50\n",
        "    moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(moving_avg, label='Recompensa promedio (media móvil)')\n",
        "    plt.plot(rewards, alpha=0.2, label='Recompensa por episodio')\n",
        "    plt.title('Entrenamiento del agente en Taxi-v3')\n",
        "    plt.xlabel('Episodio')\n",
        "    plt.ylabel('Recompensa total')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_learning_analysis(list_stats, episode_sizes):\n",
        "    # Creamos una figura con 3 subplots\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    # 1. Proporción de recompensas\n",
        "    plt.subplot(3, 1, 1)\n",
        "    indices = list(range(len(list_stats)))\n",
        "    plt.plot(indices, list_stats)\n",
        "    plt.title('Proporción de recompensas')\n",
        "    plt.xlabel('Episodio')\n",
        "    plt.ylabel('Proporción')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # 2. Tamaño de los episodios\n",
        "    plt.subplot(3, 1, 2)\n",
        "    indices = list(range(len(episode_sizes)))\n",
        "    plt.plot(indices, episode_sizes)\n",
        "    window_size = 50\n",
        "    moving_avg = np.convolve(episode_sizes, np.ones(window_size)/window_size, mode='valid')\n",
        "    plt.plot(indices[window_size-1:], moving_avg, 'r', alpha=0.5, label='Media móvil')\n",
        "    plt.title('Tamaño de los episodios')\n",
        "    plt.xlabel('Episodio')\n",
        "    plt.ylabel('Número de pasos')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # 3. Longitud del episodio en función del tiempo f(t) = len(episodiot)\n",
        "    plt.subplot(3, 1, 3)\n",
        "    cumulative_steps = np.cumsum(episode_sizes)\n",
        "    plt.plot(cumulative_steps, episode_sizes, label='Longitud del episodio')\n",
        "    # Media móvil para ver la tendencia\n",
        "    window_size = 50\n",
        "    moving_avg = np.convolve(episode_sizes, np.ones(window_size)/window_size, mode='valid')\n",
        "    plt.plot(cumulative_steps[window_size-1:], moving_avg, 'r', alpha=0.5, label='Media móvil')\n",
        "    plt.title('Longitud del episodio vs Tiempo total')\n",
        "    plt.xlabel('Tiempo total (pasos acumulados)')\n",
        "    plt.ylabel('Longitud del episodio')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lSWSl8RsiMdq"
      },
      "outputs": [],
      "source": [
        "def create_gif(frames: list, filename, fps=5):\n",
        "   \"\"\"\n",
        "   Creates a GIF animation from a list of RGBA NumPy arrays.\n",
        "   Args:\n",
        "       frames: A list of RGBA NumPy arrays representing the animation frames.\n",
        "       filename: The output filename for the GIF animation.\n",
        "       fps: The frames per second of the animation (default: 10).\n",
        "   \"\"\"\n",
        "   clip = ImageSequenceClip(frames, fps=fps)\n",
        "   clip.write_videofile(filename, fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x9OS4c9ctc5",
        "outputId": "73eec370-57c3-4857-bfde-0344429bdacc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.4452088,  0.       ], dtype=float32), {})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#@title Cargamos el entorno\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "env.reset(seed=semilla)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "ZJY7c0bcczDo",
        "outputId": "aa85b876-fe91-4147-bee5-ae9c46eff6b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-0d175055-61cf-4404-8f1a-94a1643fadd4\" class=\"ndarray_repr\"><pre>ndarray (400, 600, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-0d175055-61cf-4404-8f1a-94a1643fadd4 button').onclick = (e) => {\n",
              "        document.querySelector('#id-0d175055-61cf-4404-8f1a-94a1643fadd4').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-0d175055-61cf-4404-8f1a-94a1643fadd4 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G93Hh6iWc1g8",
        "outputId": "9d3b4a68-f0fd-47dd-ec6d-477ff05e7933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(3)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt0FvmSGc2WC",
        "outputId": "9be4a273-b10d-4d54-a992-3720230abd0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Box([-1.2  -0.07], [0.6  0.07], (2,), float32),\n",
              " array([-1.2 , -0.07], dtype=float32),\n",
              " array([0.6 , 0.07], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "env.observation_space, env.observation_space.low,  env.observation_space.high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM-RAuAydXmQ"
      },
      "source": [
        "# Tiling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hwvGxKQPdZiF"
      },
      "outputs": [],
      "source": [
        "#@title Extensión de la clase ObservationWrapper de Gymnasium para discretizar estados continuos\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "env.reset(seed=semilla)\n",
        "\n",
        "# https://gymnasium.farama.org/tutorials/gymnasium_basics/implementing_custom_wrappers/\n",
        "\n",
        "\n",
        "# Definimos una clase que hereda de gym.ObservationWrapper, la cual nos permite modificar las observaciones que devuelve el entorno.\n",
        "\n",
        "class TileCodingEnv(ObservationWrapper):\n",
        "    \"\"\"\n",
        "    TileCodingEnv es un envoltorio para un entorno Gym que aplica la técnica de Tile Coding.\n",
        "    Esta técnica discretiza observaciones continuas en múltiples rejillas (tilings) desplazadas,\n",
        "    permitiendo representar el espacio de estados de forma que se faciliten la generalización y el aprendizaje.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, bins, low, high, n=4):\n",
        "        \"\"\"\n",
        "        Inicializa el entorno env con tile coding.\n",
        "\n",
        "        Parámetros:\n",
        "        - env: entorno original de Gym.\n",
        "        - bins: array o lista con el número de intervalos (bins) que hay que particionar cada dimensión.\n",
        "        - low: array con el límite inferior para cada dimensión.\n",
        "        - high: array con el límite superior para cada dimensión.\n",
        "        - n: número de tilings (rejillas) a crear (por defecto 4).\n",
        "\n",
        "        Se llama al método _create_tilings para generar las rejillas desplazadas.\n",
        "        \"\"\"\n",
        "        super().__init__(env)  # Llama al constructor de la clase padre ObservationWrapper.\n",
        "\n",
        "        # Guardamos atributos útiles para calcular las features solo una vez.\n",
        "        self.bins = bins              # Ej.: np.array([10, 10])\n",
        "        self.n_tilings = n            # Número de tilings.\n",
        "        self.tile_size = int(np.prod(bins))  # Número de celdas en cada tiling.\n",
        "        self.last_active_features = None     # Aquí se guardarán las features activas de la última observación\n",
        "\n",
        "        self._high = high\n",
        "        self._low = low\n",
        "\n",
        "        self.tilings = self._create_tilings() # (bins, high, low, n)  # Crea y almacena las tilings.\n",
        "\n",
        "        # el vector de observación tendrá C componentes. Por ejemplo, para 2 dimensiones × 4 tilings = C = 8.\n",
        "        self.observation_space = gym.spaces.MultiDiscrete(nvec=bins.tolist()*n)\n",
        "\n",
        "\n",
        "\n",
        "    def observation(self, obs):  # Es necesario sobreescribir este método de ObservationWrapper\n",
        "        \"\"\"\n",
        "        Transforma una observación continua en una representación discreta usando tile coding.\n",
        "\n",
        "        Parámetro:\n",
        "        - obs: observación continua proveniente del entorno.\n",
        "\n",
        "        Para cada tiling en self.tilings, se determina el índice (bin) para cada dimensión usando np.digitize.\n",
        "        Se devuelve una lista de tuplas de índices, una por cada tiling.\n",
        "        Antes de retornar, se calcula y almacena en self.last_active_features el conjunto de índices\n",
        "        activos (flattened) usando _get_active_features().\n",
        "\n",
        "        Retorna:\n",
        "        - indices: lista de tuplas de índices, una por cada tiling.\n",
        "\n",
        "        \"\"\"\n",
        "        indices = []  # Lista que almacenará los índices discretizados para cada tiling.\n",
        "        for t in self.tilings:\n",
        "            # Para cada tiling 't', se calcula el índice en el que se encuentra cada componente de la observación.\n",
        "            tiling_indices = tuple(np.digitize(i, b) for i, b in zip(obs, t))\n",
        "            indices.append(tiling_indices)  # Se agrega la tupla de índices correspondiente a la tiling actual.\n",
        "\n",
        "        # Calcula y guarda las features activas a partir de los índices obtenidos.\n",
        "        self.last_active_features = self._get_active_features(indices)\n",
        "        return indices # Retorna la lista de índices de todas las tilings.\n",
        "\n",
        "\n",
        "    def _get_active_features(self, tiles):\n",
        "        \"\"\"\n",
        "        Método privado para calcular los índices (features) activos en la función aproximada.\n",
        "\n",
        "        Parámetro:\n",
        "        - tiles: lista de tuplas (una por tiling) obtenida de observation(), donde cada tupla\n",
        "          contiene los índices discretizados para cada dimensión.\n",
        "\n",
        "        La función realiza lo siguiente:\n",
        "          1. Convierte cada tupla de índices a un índice plano usando np.ravel_multi_index(tile, bins).\n",
        "            - Dado que `tile` es una tupla de índices - por ejemplo, `(3, 5)` , y\n",
        "            - dado que `bins` indica las particiones en cada dimensión - por ejemplo `bins = [10, 10]``\n",
        "            - entonces `(3, 5)` se mapea a  3*10 + 5 = 35\n",
        "            - Este índice plano identifica de forma única una celda dentro de una tiling.\n",
        "\n",
        "          2. Asigna a cada tiling un bloque distinto en el vector de parámetros, de forma que:\n",
        "             feature = (índice del tiling * tile_size) + índice plano.\n",
        "             - Por ejemplo, con dos tilings, si en ambos se selecccionara el tile `(3, 5)`\n",
        "             - Para el tiling 0 (i = 0), el flat_index será 35 (como se ha calculado antes)\n",
        "             - Pero para el tiling 1 (i=1), el flat_index será 1*100+35=135\n",
        "\n",
        "        Retorna:\n",
        "        - features: lista de índices únicos (enteros) que indican las características activas.\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        for i, tile in enumerate(tiles):\n",
        "            # Convierte la tupla 'tile' a un índice plano.\n",
        "            flat_index = np.ravel_multi_index(tile, self.bins)\n",
        "            # Asigna a cada tiling un bloque único: para el tiling i, los índices van desde i*tile_size hasta (i+1)*tile_size - 1.\n",
        "            feature = i * self.tile_size + flat_index\n",
        "            features.append(feature)\n",
        "        return features\n",
        "\n",
        "\n",
        "\n",
        "    def _create_tilings(self): # , bins, high, low, n):\n",
        "        \"\"\"\n",
        "        Crea 'n' tilings (rejillas) desplazadas para el tile coding.\n",
        "\n",
        "        Parámetros:\n",
        "        - bins: número de intervalos (bins) en cada dimensión.\n",
        "        - high: array con el límite superior para cada dimensión.\n",
        "        - low: array con el límite inferior para cada dimensión.\n",
        "        - n: número de tilings a crear.\n",
        "\n",
        "        El proceso consiste en:\n",
        "         1. Generar un vector de desplazamientos base (displacement_vector) para cada dimensión.\n",
        "         2. Para cada tiling, se ajustan los límites 'low' y 'high' añadiéndoles un pequeño desplazamiento aleatorio.\n",
        "         3. Se calculan los tamaños de los segmentos en cada dimensión (segment_sizes).\n",
        "         4. Se determinan desplazamientos específicos para cada dimensión y se aplican a los límites.\n",
        "         5. Finalmente, se generan los buckets (límites discretos) para cada dimensión usando np.linspace.\n",
        "\n",
        "        Retorna:\n",
        "        - tilings: una lista donde cada elemento es una tiling (lista de arrays de buckets para cada dimensión).\n",
        "        \"\"\"\n",
        "        # Se genera un vector de desplazamientos en cada dimensión en base a los números impares.\n",
        "        # P.e. Si hay 2 dimensiones (len(bins) == 2): np.arange(1, 2 * 2, 2) -> np.arange(1, 4, 2) devuelve [1, 3]\n",
        "        #      Si la dimensión 1 se desplaza en 1 unidad, en la dimensión 2 se desplazará en 3 unidades.\n",
        "        # P.e. Si hay 3 dimensiones (len(bins) == 3): np.arange(1, 2 * 3, 2) -> np.arange(1, 6, 2) devuelve [1, 3, 5]\n",
        "        # P.e. Si hay 4 dimensiones (len(bins) == 4): np.arange(1, 2 * 4, 2) -> np.arange(1, 8, 2) devuelve [1, 3, 5, 7]\n",
        "        # Y así sucesivamente.\n",
        "        # displacement_vector se ajusta automáticamente generando un array de números impares\n",
        "        # Estos valores se usan posteriormente para calcular los desplazamientos específicos en cada dimensión al crear las tilings (rejillas).\n",
        "        # ¿Por qué esos valores? Porque son los recomendados: los primeros números impares.\n",
        "        displacement_vector = np.arange(1, 2 * len(bins), 2)\n",
        "\n",
        "\n",
        "        tilings = []  # Lista que almacenará todas las tilings generadas.\n",
        "        for i in range(0, self.n_tilings):\n",
        "            # Para cada tiling 'i', se calculan nuevos límites 'low_i' y 'high_i' con un desplazamiento aleatorio.\n",
        "            # El desplazamiento aleatorio se basa en el 20% de los límites originales.\n",
        "            low_i = self._low  # - random.random() * 0.2 * low\n",
        "            high_i = self._high # + random.random() * 0.2 * high\n",
        "\n",
        "            # Vamos a calcular el desplazamiento específico para cada dimensión y cada mosaico.\n",
        "\n",
        "            # Antes calculamos displacement_vector, que nos indica el desplazamiento en cada dimensión.\n",
        "            # Como tenemos varios mosaicos, cada uno se tendrá que desplazar\n",
        "            # en la mismas cantidades con respecto al mosaico anterior.\n",
        "            # Esto se puede conseguir multiplicando el displacement_vector por el número de mosaico (i),\n",
        "            # pero se toma el módulo n (número total de mosaicos).\n",
        "            # De esta forma el desplazamiento de cada mosaico es diferente, dentro del rango [0, n-1]\n",
        "\n",
        "            # P.e. Para n=4 mosaicos, y dos dimensiones, los vectores de desplazamiento de cada mosaico son:\n",
        "            # i = 1: [1, 3] = [1, 3] * 1 % 4 = [1, 3] % 4\n",
        "            # i = 2: [2, 2] = [1, 3] * 2 % 4 = [2, 6] % 4\n",
        "            # i = 3: [3, 1] = [1, 3] * 3 % 4 = [3, 9] % 4\n",
        "            # i = 4: [0, 0] = [1, 3] * 4 % 4 = [4, 12] % 4\n",
        "            displacements = displacement_vector * i % self.n_tilings\n",
        "\n",
        "            # Pero hay que escalar el desplazamiento a unidades reales en cada dimensión.\n",
        "            # Para ello necesitamos calcular el tamaño de cada segmento (intervalo) en cada dimensión.\n",
        "            segment_sizes = (high_i - low_i) / self.bins\n",
        "\n",
        "            # Entonces usamos una fracción del tamaño del segmento para desplazar cada mosaico.\n",
        "            # La fracción del tamaño del segmento viene dado por el tamaño del segmento dividido por el número de mosaicos.\n",
        "            # Por ejemplo, si el tamaño de la celda es 0.5 en la primera dimensión y se consideran n=4 mosaicos, la fracción es 0.5/4=0.125\n",
        "            # Según se ha calculado anteriormente, en el vector de desplazamiento,\n",
        "            # la primera dimensión se desplaza en 1, 2, 3 y 0 unidades para los mosaicos 1, 2, 3, y 4, respectivamente.\n",
        "            # Como la unidad es 0.125, entonces la primera dimensión de cada mosaico se desplaza en las cantidades:\n",
        "            # 0.125 = 1 * 0.125,  0.25 = 2 * 0.125, 0.375 = 3 * 0.125, y  0 = 0 * 0.125.\n",
        "            # Lo mismo se haría con el resto de dimensiones. En forma vectorial:\n",
        "            # Es decir, el desplazamiento de cada mosaico en la primera dimensión es:\n",
        "            # Tiling 1, [1, 3]: [1 * 0.125, 3 * 0.05] = [0.125, 0.15]\n",
        "            # Tiling 2, [2, 2]: [2 * 0.125, 2 * 0.05] = [0.25, 0.10]\n",
        "            # Tiling 3, [3, 1]: [3 * 0.125, 1 * 0.05] = [0.375, 0.05]\n",
        "            # Tiling 4  [0, 0]: [0 * 0.125, 0 * 0.05] = [0, 0]\n",
        "            displacements = displacements * (segment_sizes / self.n_tilings)\n",
        "\n",
        "            dlow_i = low_i + displacements\n",
        "            dhigh_i = high_i + displacements\n",
        "            # print(f\"Tiling {i}: Se aplican los desplazamientos {displacements} a los límites inferiores {low_i}->{dlow_i} y superiores {high_i}->{dhigh_i}.\")\n",
        "\n",
        "            # Para cada dimensión, se crean los buckets que dividen el intervalo de low_i a high_i en 'bins' partes,\n",
        "            # generando 'l-1' puntos (límites) para cada dimensión.\n",
        "            buckets_i = [np.linspace(j, k, l - 1) for j, k, l in zip(dlow_i, dhigh_i, bins)]\n",
        "\n",
        "            # Se añade la tiling actual (lista de buckets para cada dimensión) a la lista de tilings.\n",
        "            tilings.append(buckets_i)\n",
        "\n",
        "        return tilings  # Retorna la lista completa de tilings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jx5i4UQtdaLV"
      },
      "outputs": [],
      "source": [
        "#@title Generamos  mosaicos (tilings) con varios intervalos por dimensión. Al nuevo espacio lo llamaremos *tcenv*\n",
        "tilings = 10  # Número de mosaicos\n",
        "bins = np.array([20, 20])  # Número de intervalos en cada dimensión. 20 en cada una\n",
        "low = env.observation_space.low\n",
        "high = env.observation_space.high\n",
        "tcenv = TileCodingEnv(env, bins=bins, low=low, high=high, n=tilings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU-TnwcUdnhJ",
        "outputId": "fbe3efe8-93d6-4e71-ba73-186d2757aa11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El espacio de observaciones original es: Box([-1.2  -0.07], [0.6  0.07], (2,), float32), \n",
            "Un estado para este espacio original es: (array([-0.4457913 , -0.00058252], dtype=float32), -1.0, False, False, {})\n",
            "\n",
            "El espacio de estados modificado es: MultiDiscrete([20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20]). Mosaicos: 10 \n",
            "Un estado para este nuevo espacio es: [(8, 9), (8, 9), (8, 9), (8, 9), (8, 9), (8, 9), (8, 9), (7, 9), (7, 9), (7, 9)] \n",
            "Cada pareja es la 'celda' correspondiente a cada mosaico\n",
            "Las caracterísiticas observadas han sido [169, 569, 969, 1369, 1769, 2169, 2569, 2949, 3349, 3749] sobre 4000 parámetros\n"
          ]
        }
      ],
      "source": [
        "#@title Comparamos el entorno original con el entorno con estados agregados\n",
        "\n",
        "print(f\"El espacio de observaciones original es: {env.observation_space}, \\n\\\n",
        "Un estado para este espacio original es: {env.step(env.action_space.sample())}\")\n",
        "print(f\"\\nEl espacio de estados modificado es: {tcenv.observation_space}. Mosaicos: {tcenv.n_tilings} \\n\\\n",
        "Un estado para este nuevo espacio es: {tcenv.step(tcenv.action_space.sample())[0]} \\n\\\n",
        "Cada pareja es la 'celda' correspondiente a cada mosaico\")\n",
        "print(f\"Las caracterísiticas observadas han sido {tcenv.last_active_features} sobre {np.prod(tcenv.bins)*tcenv.n_tilings} parámetros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJD9_kagduzw"
      },
      "source": [
        "# Sarsa Semi-Gradiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xt-Yrob8duZB"
      },
      "outputs": [],
      "source": [
        "class SarsaSemiGradientAgent:\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.999, min_epsilon=0.01):\n",
        "        \"\"\"\n",
        "        Inicializa el agente SARSA con gradiente semilineal y Tile Coding.\n",
        "\n",
        "        Parámetros:\n",
        "        - env: Entorno de Gymnasium con Tile Coding.\n",
        "        - alpha: Tasa de aprendizaje.\n",
        "        - gamma: Factor de descuento (importancia de recompensas futuras).\n",
        "        - epsilon: Probabilidad inicial de exploración (ε-greedy).\n",
        "        - epsilon_decay: Factor de reducción de epsilon por episodio.\n",
        "        - min_epsilon: Límite inferior de epsilon para evitar exploración excesiva.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "        # Número de acciones posibles en el entorno\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "        # Número total de características en la codificación de tiles\n",
        "        self.num_features = env.n_tilings * np.prod(env.bins)\n",
        "\n",
        "        # Matriz de pesos inicializados en 0, de tamaño [n_features, n_actions]\n",
        "        self.weights = np.zeros((self.num_features, self.num_actions))\n",
        "\n",
        "    def get_q(self, active_features, action):\n",
        "        \"\"\"\n",
        "        Calcula el valor de Q(s, a) como la suma de los pesos de las features activas.\n",
        "\n",
        "        Parámetros:\n",
        "        - active_features: Lista de índices de características activas en el estado s.\n",
        "        - action: Acción para la cual se calcula Q(s, a).\n",
        "\n",
        "        Retorna:\n",
        "        - Valor de Q(s, a) sumando los pesos correspondientes a las características activas.\n",
        "        \"\"\"\n",
        "        return np.sum(self.weights[active_features, action])\n",
        "\n",
        "    def choose_action(self, active_features):\n",
        "        \"\"\"\n",
        "        Selecciona una acción usando la estrategia ε-greedy.\n",
        "\n",
        "        Parámetros:\n",
        "        - active_features: Lista de índices de características activas en el estado actual.\n",
        "\n",
        "        Retorna:\n",
        "        - Acción seleccionada (exploratoria o explotadora).\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Exploración: Selecciona una acción aleatoria con probabilidad ε\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            # Explotación: Selecciona la acción con el mayor valor Q estimado\n",
        "            q_values = [np.sum(self.weights[active_features, action]) for action in range(self.num_actions)]\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Reduce el valor de epsilon para disminuir la exploración con el tiempo.\n",
        "        Se asegura de que epsilon no caiga por debajo de min_epsilon.\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n",
        "\n",
        "    def update(self, active_features, active_features_next, a, a_next, reward, done, truncated):\n",
        "        \"\"\"\n",
        "        Realiza la actualización de los pesos usando la ecuación de actualización de SARSA.\n",
        "\n",
        "        Parámetros:\n",
        "        - active_features: Lista de características activas en el estado actual.\n",
        "        - active_features_next: Lista de características activas en el próximo estado.\n",
        "        - a: Acción tomada en el estado actual.\n",
        "        - a_next: Acción tomada en el próximo estado.\n",
        "        - reward: Recompensa obtenida por tomar la acción a en el estado actual.\n",
        "        - done: Indica si se llegó al estado terminal.\n",
        "        - truncated: Indica si el episodio se truncó por un límite de pasos.\n",
        "\n",
        "        \"\"\"\n",
        "        # Calcular Q(s,a) para el estado actual y la acción tomada\n",
        "        q_sa = self.get_q(active_features, a)\n",
        "\n",
        "        # Si no es estado terminal, calcular Q(s',a')\n",
        "        if not (done or truncated):\n",
        "            q_sap = self.get_q(active_features_next, a_next)\n",
        "            delta = reward + self.gamma * q_sap - q_sa  # Error de TD\n",
        "        else:\n",
        "            delta = reward - q_sa  # En estado terminal, Q(s', a') no contribuye\n",
        "\n",
        "        # Actualizar los pesos solo en las features activas para la acción 'a'\n",
        "        for i in active_features:\n",
        "            self.weights[i, a] += self.alpha * delta  # Regla de actualización de SARSA\n",
        "\n",
        "        # Reducir epsilon después de cada actualización\n",
        "        self.decay_epsilon()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JMeR6lhBfyqk"
      },
      "outputs": [],
      "source": [
        "# Hiperparámetros\n",
        "num_episodes = 100000  # Número total de episodios de entrenamiento\n",
        "GAMMA = 0.99  # Factor de descuento: importancia de las recompensas futuras (0 a 1)\n",
        "ALPHA = 0.1  # Tasa de aprendizaje: cuánto se ajustan los pesos en cada actualización\n",
        "EPSILON = 1.0  # Probabilidad de exploración inicial (epsilon-greedy)\n",
        "EPSILON_DECAY = 0.999  # Factor de decaimiento de epsilon: reduce la exploración con el tiempo\n",
        "MIN_EPSILON = 0.01  # Valor mínimo de epsilon: asegura una mínima exploración continua\n",
        "max_steps = 1000  # Número máximo de pasos por episodio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3Kpg13OXgJA0"
      },
      "outputs": [],
      "source": [
        "# Inicializar el agente\n",
        "agent = SarsaSemiGradientAgent(\n",
        "    env=tcenv,\n",
        "    alpha=ALPHA,\n",
        "    gamma=GAMMA,\n",
        "    epsilon=EPSILON,\n",
        "    epsilon_decay=EPSILON_DECAY,\n",
        "    min_epsilon=MIN_EPSILON\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHea_OGO0ZAH",
        "outputId": "bfb7c37c-b884-4170-83b5-a0a01c61d8fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100000, total reward: -200.0\n",
            "Episode 2/100000, total reward: -200.0\n",
            "Episode 3/100000, total reward: -200.0\n",
            "Episode 4/100000, total reward: -200.0\n",
            "Episode 5/100000, total reward: -200.0\n",
            "Episode 6/100000, total reward: -200.0\n",
            "Episode 7/100000, total reward: -200.0\n",
            "Episode 8/100000, total reward: -200.0\n",
            "Episode 9/100000, total reward: -200.0\n",
            "Episode 10/100000, total reward: -200.0\n",
            "Episode 11/100000, total reward: -200.0\n",
            "Episode 12/100000, total reward: -200.0\n",
            "Episode 13/100000, total reward: -200.0\n",
            "Episode 14/100000, total reward: -200.0\n",
            "Episode 15/100000, total reward: -200.0\n",
            "Episode 16/100000, total reward: -200.0\n",
            "Episode 17/100000, total reward: -200.0\n",
            "Episode 18/100000, total reward: -200.0\n",
            "Episode 19/100000, total reward: -200.0\n",
            "Episode 20/100000, total reward: -200.0\n",
            "Episode 21/100000, total reward: -200.0\n",
            "Episode 22/100000, total reward: -200.0\n",
            "Episode 23/100000, total reward: -200.0\n",
            "Episode 24/100000, total reward: -200.0\n",
            "Episode 25/100000, total reward: -200.0\n",
            "Episode 26/100000, total reward: -200.0\n",
            "Episode 27/100000, total reward: -200.0\n",
            "Episode 28/100000, total reward: -200.0\n",
            "Episode 29/100000, total reward: -200.0\n",
            "Episode 30/100000, total reward: -200.0\n",
            "Episode 31/100000, total reward: -200.0\n",
            "Episode 32/100000, total reward: -200.0\n",
            "Episode 33/100000, total reward: -200.0\n",
            "Episode 34/100000, total reward: -200.0\n",
            "Episode 35/100000, total reward: -200.0\n",
            "Episode 36/100000, total reward: -200.0\n",
            "Episode 37/100000, total reward: -200.0\n",
            "Episode 38/100000, total reward: -200.0\n",
            "Episode 39/100000, total reward: -200.0\n",
            "Episode 40/100000, total reward: -200.0\n",
            "Episode 41/100000, total reward: -200.0\n",
            "Episode 42/100000, total reward: -200.0\n",
            "Episode 43/100000, total reward: -200.0\n",
            "Episode 44/100000, total reward: -200.0\n",
            "Episode 45/100000, total reward: -200.0\n",
            "Episode 46/100000, total reward: -200.0\n",
            "Episode 47/100000, total reward: -200.0\n",
            "Episode 48/100000, total reward: -200.0\n",
            "Episode 49/100000, total reward: -200.0\n",
            "Episode 50/100000, total reward: -200.0\n",
            "Episode 51/100000, total reward: -200.0\n",
            "Episode 52/100000, total reward: -200.0\n",
            "Episode 53/100000, total reward: -200.0\n",
            "Episode 54/100000, total reward: -200.0\n",
            "Episode 55/100000, total reward: -200.0\n",
            "Episode 56/100000, total reward: -200.0\n",
            "Episode 57/100000, total reward: -200.0\n",
            "Episode 58/100000, total reward: -200.0\n",
            "Episode 59/100000, total reward: -200.0\n",
            "Episode 60/100000, total reward: -200.0\n",
            "Episode 61/100000, total reward: -200.0\n",
            "Episode 62/100000, total reward: -200.0\n",
            "Episode 63/100000, total reward: -200.0\n",
            "Episode 64/100000, total reward: -200.0\n",
            "Episode 65/100000, total reward: -179.0\n",
            "Episode 66/100000, total reward: -200.0\n",
            "Episode 67/100000, total reward: -200.0\n",
            "Episode 68/100000, total reward: -200.0\n",
            "Episode 69/100000, total reward: -169.0\n",
            "Episode 70/100000, total reward: -200.0\n",
            "Episode 71/100000, total reward: -174.0\n",
            "Episode 72/100000, total reward: -200.0\n",
            "Episode 73/100000, total reward: -159.0\n",
            "Episode 74/100000, total reward: -200.0\n",
            "Episode 75/100000, total reward: -168.0\n",
            "Episode 76/100000, total reward: -200.0\n",
            "Episode 77/100000, total reward: -200.0\n",
            "Episode 78/100000, total reward: -200.0\n",
            "Episode 79/100000, total reward: -200.0\n",
            "Episode 80/100000, total reward: -160.0\n",
            "Episode 81/100000, total reward: -200.0\n",
            "Episode 82/100000, total reward: -200.0\n",
            "Episode 83/100000, total reward: -197.0\n",
            "Episode 84/100000, total reward: -185.0\n",
            "Episode 85/100000, total reward: -190.0\n",
            "Episode 86/100000, total reward: -194.0\n",
            "Episode 87/100000, total reward: -165.0\n",
            "Episode 88/100000, total reward: -162.0\n",
            "Episode 89/100000, total reward: -160.0\n",
            "Episode 90/100000, total reward: -173.0\n",
            "Episode 91/100000, total reward: -162.0\n",
            "Episode 92/100000, total reward: -172.0\n",
            "Episode 93/100000, total reward: -200.0\n",
            "Episode 94/100000, total reward: -155.0\n",
            "Episode 95/100000, total reward: -200.0\n",
            "Episode 96/100000, total reward: -200.0\n",
            "Episode 97/100000, total reward: -200.0\n",
            "Episode 98/100000, total reward: -200.0\n",
            "Episode 99/100000, total reward: -200.0\n",
            "Episode 100/100000, total reward: -200.0\n",
            "Episode 101/100000, total reward: -200.0\n",
            "Episode 102/100000, total reward: -200.0\n",
            "Episode 103/100000, total reward: -200.0\n",
            "Episode 104/100000, total reward: -200.0\n",
            "Episode 105/100000, total reward: -200.0\n",
            "Episode 106/100000, total reward: -200.0\n",
            "Episode 107/100000, total reward: -200.0\n",
            "Episode 108/100000, total reward: -200.0\n",
            "Episode 109/100000, total reward: -200.0\n",
            "Episode 110/100000, total reward: -200.0\n",
            "Episode 111/100000, total reward: -160.0\n",
            "Episode 112/100000, total reward: -155.0\n",
            "Episode 113/100000, total reward: -200.0\n",
            "Episode 114/100000, total reward: -200.0\n",
            "Episode 115/100000, total reward: -200.0\n",
            "Episode 116/100000, total reward: -200.0\n",
            "Episode 117/100000, total reward: -200.0\n",
            "Episode 118/100000, total reward: -166.0\n",
            "Episode 119/100000, total reward: -200.0\n",
            "Episode 120/100000, total reward: -156.0\n",
            "Episode 121/100000, total reward: -172.0\n",
            "Episode 122/100000, total reward: -200.0\n",
            "Episode 123/100000, total reward: -200.0\n",
            "Episode 124/100000, total reward: -175.0\n",
            "Episode 125/100000, total reward: -200.0\n",
            "Episode 126/100000, total reward: -200.0\n",
            "Episode 127/100000, total reward: -200.0\n",
            "Episode 128/100000, total reward: -162.0\n",
            "Episode 129/100000, total reward: -200.0\n",
            "Episode 130/100000, total reward: -200.0\n",
            "Episode 131/100000, total reward: -200.0\n",
            "Episode 132/100000, total reward: -200.0\n",
            "Episode 133/100000, total reward: -187.0\n",
            "Episode 134/100000, total reward: -177.0\n",
            "Episode 135/100000, total reward: -200.0\n",
            "Episode 136/100000, total reward: -200.0\n",
            "Episode 137/100000, total reward: -200.0\n",
            "Episode 138/100000, total reward: -200.0\n",
            "Episode 139/100000, total reward: -146.0\n",
            "Episode 140/100000, total reward: -157.0\n",
            "Episode 141/100000, total reward: -200.0\n",
            "Episode 142/100000, total reward: -171.0\n",
            "Episode 143/100000, total reward: -200.0\n",
            "Episode 144/100000, total reward: -171.0\n",
            "Episode 145/100000, total reward: -164.0\n",
            "Episode 146/100000, total reward: -163.0\n",
            "Episode 147/100000, total reward: -160.0\n",
            "Episode 148/100000, total reward: -162.0\n",
            "Episode 149/100000, total reward: -165.0\n",
            "Episode 150/100000, total reward: -200.0\n",
            "Episode 151/100000, total reward: -200.0\n",
            "Episode 152/100000, total reward: -200.0\n",
            "Episode 153/100000, total reward: -200.0\n",
            "Episode 154/100000, total reward: -200.0\n",
            "Episode 155/100000, total reward: -178.0\n",
            "Episode 156/100000, total reward: -195.0\n",
            "Episode 157/100000, total reward: -183.0\n",
            "Episode 158/100000, total reward: -183.0\n",
            "Episode 159/100000, total reward: -174.0\n",
            "Episode 160/100000, total reward: -160.0\n",
            "Episode 161/100000, total reward: -200.0\n",
            "Episode 162/100000, total reward: -150.0\n",
            "Episode 163/100000, total reward: -154.0\n",
            "Episode 164/100000, total reward: -149.0\n",
            "Episode 165/100000, total reward: -200.0\n",
            "Episode 166/100000, total reward: -200.0\n",
            "Episode 167/100000, total reward: -200.0\n",
            "Episode 168/100000, total reward: -159.0\n",
            "Episode 169/100000, total reward: -161.0\n",
            "Episode 170/100000, total reward: -159.0\n",
            "Episode 171/100000, total reward: -156.0\n",
            "Episode 172/100000, total reward: -153.0\n",
            "Episode 173/100000, total reward: -158.0\n",
            "Episode 174/100000, total reward: -157.0\n",
            "Episode 175/100000, total reward: -161.0\n",
            "Episode 176/100000, total reward: -200.0\n",
            "Episode 177/100000, total reward: -149.0\n",
            "Episode 178/100000, total reward: -200.0\n",
            "Episode 179/100000, total reward: -200.0\n",
            "Episode 180/100000, total reward: -200.0\n",
            "Episode 181/100000, total reward: -187.0\n",
            "Episode 182/100000, total reward: -200.0\n",
            "Episode 183/100000, total reward: -157.0\n",
            "Episode 184/100000, total reward: -148.0\n",
            "Episode 185/100000, total reward: -168.0\n",
            "Episode 186/100000, total reward: -150.0\n",
            "Episode 187/100000, total reward: -156.0\n",
            "Episode 188/100000, total reward: -154.0\n",
            "Episode 189/100000, total reward: -153.0\n",
            "Episode 190/100000, total reward: -156.0\n",
            "Episode 191/100000, total reward: -158.0\n",
            "Episode 192/100000, total reward: -158.0\n",
            "Episode 193/100000, total reward: -158.0\n",
            "Episode 194/100000, total reward: -170.0\n",
            "Episode 195/100000, total reward: -200.0\n",
            "Episode 196/100000, total reward: -200.0\n",
            "Episode 197/100000, total reward: -200.0\n",
            "Episode 198/100000, total reward: -200.0\n",
            "Episode 199/100000, total reward: -200.0\n",
            "Episode 200/100000, total reward: -200.0\n",
            "Episode 201/100000, total reward: -190.0\n",
            "Episode 202/100000, total reward: -165.0\n",
            "Episode 203/100000, total reward: -184.0\n",
            "Episode 204/100000, total reward: -190.0\n",
            "Episode 205/100000, total reward: -185.0\n",
            "Episode 206/100000, total reward: -194.0\n",
            "Episode 207/100000, total reward: -179.0\n",
            "Episode 208/100000, total reward: -186.0\n",
            "Episode 209/100000, total reward: -188.0\n",
            "Episode 210/100000, total reward: -186.0\n",
            "Episode 211/100000, total reward: -185.0\n",
            "Episode 212/100000, total reward: -180.0\n",
            "Episode 213/100000, total reward: -153.0\n",
            "Episode 214/100000, total reward: -200.0\n",
            "Episode 215/100000, total reward: -200.0\n",
            "Episode 216/100000, total reward: -155.0\n",
            "Episode 217/100000, total reward: -158.0\n",
            "Episode 218/100000, total reward: -145.0\n",
            "Episode 219/100000, total reward: -143.0\n",
            "Episode 220/100000, total reward: -161.0\n",
            "Episode 221/100000, total reward: -148.0\n",
            "Episode 222/100000, total reward: -161.0\n",
            "Episode 223/100000, total reward: -145.0\n",
            "Episode 224/100000, total reward: -152.0\n",
            "Episode 225/100000, total reward: -146.0\n",
            "Episode 226/100000, total reward: -155.0\n",
            "Episode 227/100000, total reward: -147.0\n",
            "Episode 228/100000, total reward: -148.0\n",
            "Episode 229/100000, total reward: -152.0\n",
            "Episode 230/100000, total reward: -160.0\n",
            "Episode 231/100000, total reward: -160.0\n",
            "Episode 232/100000, total reward: -139.0\n",
            "Episode 233/100000, total reward: -145.0\n",
            "Episode 234/100000, total reward: -153.0\n",
            "Episode 235/100000, total reward: -154.0\n",
            "Episode 236/100000, total reward: -142.0\n",
            "Episode 237/100000, total reward: -153.0\n",
            "Episode 238/100000, total reward: -162.0\n",
            "Episode 239/100000, total reward: -153.0\n",
            "Episode 240/100000, total reward: -149.0\n",
            "Episode 241/100000, total reward: -153.0\n",
            "Episode 242/100000, total reward: -163.0\n",
            "Episode 243/100000, total reward: -162.0\n",
            "Episode 244/100000, total reward: -160.0\n",
            "Episode 245/100000, total reward: -158.0\n",
            "Episode 246/100000, total reward: -158.0\n",
            "Episode 247/100000, total reward: -142.0\n",
            "Episode 248/100000, total reward: -153.0\n",
            "Episode 249/100000, total reward: -149.0\n",
            "Episode 250/100000, total reward: -159.0\n",
            "Episode 251/100000, total reward: -153.0\n",
            "Episode 252/100000, total reward: -152.0\n",
            "Episode 253/100000, total reward: -159.0\n",
            "Episode 254/100000, total reward: -152.0\n",
            "Episode 255/100000, total reward: -150.0\n",
            "Episode 256/100000, total reward: -145.0\n",
            "Episode 257/100000, total reward: -152.0\n",
            "Episode 258/100000, total reward: -155.0\n",
            "Episode 259/100000, total reward: -153.0\n",
            "Episode 260/100000, total reward: -154.0\n",
            "Episode 261/100000, total reward: -152.0\n",
            "Episode 262/100000, total reward: -151.0\n",
            "Episode 263/100000, total reward: -147.0\n",
            "Episode 264/100000, total reward: -141.0\n",
            "Episode 265/100000, total reward: -151.0\n",
            "Episode 266/100000, total reward: -155.0\n",
            "Episode 267/100000, total reward: -135.0\n",
            "Episode 268/100000, total reward: -146.0\n",
            "Episode 269/100000, total reward: -153.0\n",
            "Episode 270/100000, total reward: -169.0\n",
            "Episode 271/100000, total reward: -183.0\n",
            "Episode 272/100000, total reward: -120.0\n",
            "Episode 273/100000, total reward: -200.0\n",
            "Episode 274/100000, total reward: -170.0\n",
            "Episode 275/100000, total reward: -164.0\n",
            "Episode 276/100000, total reward: -183.0\n",
            "Episode 277/100000, total reward: -189.0\n",
            "Episode 278/100000, total reward: -184.0\n",
            "Episode 279/100000, total reward: -165.0\n",
            "Episode 280/100000, total reward: -146.0\n",
            "Episode 281/100000, total reward: -165.0\n",
            "Episode 282/100000, total reward: -146.0\n",
            "Episode 283/100000, total reward: -157.0\n",
            "Episode 284/100000, total reward: -154.0\n",
            "Episode 285/100000, total reward: -160.0\n",
            "Episode 286/100000, total reward: -153.0\n",
            "Episode 287/100000, total reward: -160.0\n",
            "Episode 288/100000, total reward: -150.0\n",
            "Episode 289/100000, total reward: -144.0\n",
            "Episode 290/100000, total reward: -200.0\n",
            "Episode 291/100000, total reward: -152.0\n",
            "Episode 292/100000, total reward: -200.0\n",
            "Episode 293/100000, total reward: -154.0\n",
            "Episode 294/100000, total reward: -155.0\n",
            "Episode 295/100000, total reward: -150.0\n",
            "Episode 296/100000, total reward: -152.0\n",
            "Episode 297/100000, total reward: -152.0\n",
            "Episode 298/100000, total reward: -162.0\n",
            "Episode 299/100000, total reward: -149.0\n",
            "Episode 300/100000, total reward: -150.0\n",
            "Episode 301/100000, total reward: -147.0\n",
            "Episode 302/100000, total reward: -144.0\n",
            "Episode 303/100000, total reward: -200.0\n",
            "Episode 304/100000, total reward: -154.0\n",
            "Episode 305/100000, total reward: -154.0\n",
            "Episode 306/100000, total reward: -148.0\n",
            "Episode 307/100000, total reward: -146.0\n",
            "Episode 308/100000, total reward: -146.0\n",
            "Episode 309/100000, total reward: -155.0\n",
            "Episode 310/100000, total reward: -149.0\n",
            "Episode 311/100000, total reward: -153.0\n",
            "Episode 312/100000, total reward: -200.0\n",
            "Episode 313/100000, total reward: -152.0\n",
            "Episode 314/100000, total reward: -152.0\n",
            "Episode 315/100000, total reward: -146.0\n",
            "Episode 316/100000, total reward: -154.0\n",
            "Episode 317/100000, total reward: -146.0\n",
            "Episode 318/100000, total reward: -169.0\n",
            "Episode 319/100000, total reward: -144.0\n",
            "Episode 320/100000, total reward: -150.0\n",
            "Episode 321/100000, total reward: -149.0\n",
            "Episode 322/100000, total reward: -164.0\n",
            "Episode 323/100000, total reward: -182.0\n",
            "Episode 324/100000, total reward: -163.0\n",
            "Episode 325/100000, total reward: -161.0\n",
            "Episode 326/100000, total reward: -164.0\n",
            "Episode 327/100000, total reward: -93.0\n",
            "Episode 328/100000, total reward: -166.0\n",
            "Episode 329/100000, total reward: -174.0\n",
            "Episode 330/100000, total reward: -169.0\n",
            "Episode 331/100000, total reward: -181.0\n",
            "Episode 332/100000, total reward: -151.0\n",
            "Episode 333/100000, total reward: -158.0\n",
            "Episode 334/100000, total reward: -146.0\n",
            "Episode 335/100000, total reward: -146.0\n",
            "Episode 336/100000, total reward: -145.0\n",
            "Episode 337/100000, total reward: -188.0\n",
            "Episode 338/100000, total reward: -152.0\n"
          ]
        }
      ],
      "source": [
        "# --- Bucle principal de entrenamiento con semigradiente SARSA ---\n",
        "rewards_per_episode = []  # Para guardar la recompensa total de cada episodio\n",
        "episodes_sizes = []\n",
        "for episode in range(num_episodes):\n",
        "    # Resetear el entorno (Gymnasium devuelve (obs, info))\n",
        "    obs, info = tcenv.reset(seed=semilla)\n",
        "    # El método observation() del wrapper actualiza internamente tcenv.last_active_features.\n",
        "    active_features = tcenv.last_active_features\n",
        "    # Seleccionar acción inicial usando epsilon-greedy\n",
        "    a = agent.choose_action(active_features)\n",
        "\n",
        "    total_reward = 0.0\n",
        "    done = False\n",
        "\n",
        "    for t in range(max_steps):\n",
        "        # Ejecutar la acción 'a' y obtener la siguiente observación\n",
        "        obs_next, reward, done, truncated, info = tcenv.step(a)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Después de step, tcenv.last_active_features se actualiza para el nuevo estado s'\n",
        "        active_features_next = tcenv.last_active_features\n",
        "        # Seleccionar la siguiente acción a' (si el episodio continúa)\n",
        "        if not (done or truncated):\n",
        "            a_next = agent.choose_action(active_features_next)\n",
        "        else:\n",
        "            a_next = None  # No se usa si es terminal\n",
        "\n",
        "        agent.update(active_features, active_features_next, a, a_next,reward, done, truncated)\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "        # Actualiza estado y acción para el siguiente paso\n",
        "        active_features = active_features_next\n",
        "        a = a_next\n",
        "\n",
        "    rewards_per_episode.append(total_reward)\n",
        "    episodes_sizes.append(t + 1)  # Agregar el tamaño del episodio a la lista\n",
        "    print(f\"Episode {episode+1}/{num_episodes}, total reward: {total_reward}\")\n",
        "\n",
        "# Después de entrenar, puedes evaluar la política (por ejemplo, sin exploración)\n",
        "avg_return = np.mean(rewards_per_episode)\n",
        "print(f\"Average return over {num_episodes} episodes: {avg_return}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXMDoTH6hPNn"
      },
      "outputs": [],
      "source": [
        "plot_learning_analysis(rewards_per_episode, episodes_sizes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vRO611NhTLr"
      },
      "outputs": [],
      "source": [
        "# Obtener la política óptima, las acciones y los frames utilizando la función pi_star_from_Q\n",
        "# Se pasa agent.weights en lugar de agent.Q porque estamos usando Sarsa Semi-Gradiente\n",
        "pi, actions, frames = pi_star_from_Q(tcenv, agent.weights)\n",
        "\n",
        "# Renderizar el entorno para obtener la imagen final\n",
        "img = tcenv.render()\n",
        "\n",
        "# Agregar la imagen final a la lista de frames\n",
        "frames.append(img)\n",
        "\n",
        "# Imprimir la política óptima, las acciones y la representación del entorno\n",
        "print(\"Política óptima obtenida\\n\", pi, f\"\\n Acciones {actions} \\n Para el siguiente grid\\n\", tcenv.render())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B0TPit8iY4z"
      },
      "outputs": [],
      "source": [
        "create_gif(frames, \"SarsaSemiGradiente.mp4\", fps=24)  # saves the GIF locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiyMnVRdxnUi"
      },
      "source": [
        "# Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4uwpAO5ySyq"
      },
      "outputs": [],
      "source": [
        "class QLearning:\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.999, min_epsilon=0.01):\n",
        "        \"\"\"\n",
        "        Inicializa el agente de Q-Learning con Tile Coding.\n",
        "\n",
        "        Parámetros:\n",
        "        - env: Entorno de Gymnasium con codificación de tiles.\n",
        "        - alpha: Tasa de aprendizaje.\n",
        "        - gamma: Factor de descuento (importancia de recompensas futuras).\n",
        "        - epsilon: Probabilidad inicial de exploración (ε-greedy).\n",
        "        - epsilon_decay: Factor de reducción de epsilon por episodio.\n",
        "        - min_epsilon: Límite inferior de epsilon para evitar exploración excesiva.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "        # Número de acciones posibles en el entorno\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "        # Número total de características en la codificación de tiles\n",
        "        self.num_features = env.n_tilings * np.prod(env.bins)\n",
        "\n",
        "        # Inicializa la tabla Q con valores aleatorios pequeños para evitar sesgos iniciales\n",
        "        self.Q = np.random.uniform(low=-1, high=1, size=(self.num_features, self.num_actions))\n",
        "\n",
        "    def q_value(self, active_features, a):\n",
        "        \"\"\"\n",
        "        Calcula el valor de Q(s,a) como la suma de los pesos para los índices activos.\n",
        "\n",
        "        Parámetros:\n",
        "        - active_features: Lista de índices de características activas para el estado s.\n",
        "        - a: Acción seleccionada.\n",
        "\n",
        "        Retorna:\n",
        "        - Valor de Q(s,a) sumando los valores de las características activas para la acción a.\n",
        "        \"\"\"\n",
        "        return self.Q[active_features, a].sum()\n",
        "\n",
        "    def compute_q_values(self, active_features, num_actions):\n",
        "        \"\"\"\n",
        "        Calcula Q(s,a) para todas las acciones posibles en el estado actual.\n",
        "\n",
        "        Parámetros:\n",
        "        - active_features: Lista de índices de características activas.\n",
        "        - num_actions: Número total de acciones en el entorno.\n",
        "\n",
        "        Retorna:\n",
        "        - Un array 1D con los valores Q(s,a) para cada acción a.\n",
        "        \"\"\"\n",
        "        q_vals = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            q_vals[a] = self.q_value(active_features, a)  # Calcula Q(s,a) para cada acción\n",
        "        return q_vals\n",
        "\n",
        "    def choose_action(self, active_features):\n",
        "        \"\"\"\n",
        "        Selecciona una acción usando la estrategia ε-greedy.\n",
        "\n",
        "        Parámetros:\n",
        "        - active_features: Lista de índices de características activas del estado actual.\n",
        "\n",
        "        Retorna:\n",
        "        - Acción seleccionada (exploratoria o explotadora).\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Exploración: Selecciona una acción aleatoria con probabilidad ε\n",
        "            action = np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            # Explotación: Selecciona la acción con el mayor valor Q estimado\n",
        "            action = np.argmax(self.compute_q_values(active_features, self.num_actions))\n",
        "        return action\n",
        "\n",
        "    def update(self, active_features, active_features_next, a, reward):\n",
        "        \"\"\"\n",
        "        Actualiza los valores de la función Q utilizando la ecuación de Bellman.\n",
        "\n",
        "        Parámetros:\n",
        "        - active_features: Lista de índices de características activas en el estado actual.\n",
        "        - active_features_next: Lista de índices de características activas en el próximo estado.\n",
        "        - a: Acción tomada en el estado actual.\n",
        "        - reward: Recompensa recibida después de tomar la acción a.\n",
        "        \"\"\"\n",
        "        # Calcula la actualización de Q usando la regla de actualización de Q-Learning\n",
        "        self.Q[active_features, a] = (\n",
        "            self.Q[active_features, a]  # Valor Q actual\n",
        "            + self.alpha * (\n",
        "                reward  # Recompensa inmediata\n",
        "                + self.gamma * np.max(self.Q[active_features_next, :])  # Mejor Q futuro\n",
        "                - self.Q[active_features, a]  # Diferencia con el valor Q actual\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Reduce el valor de epsilon para disminuir la exploración con el tiempo.\n",
        "        Se asegura de que epsilon no caiga por debajo de min_epsilon.\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.min_epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5DcZVw5-lOo"
      },
      "outputs": [],
      "source": [
        "# Hiperparámetros\n",
        "num_episodes = 5000  # Número total de episodios de entrenamiento\n",
        "GAMMA = 0.95  # Factor de descuento: importancia de las recompensas futuras (0 a 1)\n",
        "ALPHA = 0.1  # Tasa de aprendizaje: cuánto se ajustan los valores Q en cada actualización\n",
        "EPSILON = 1.0  # Probabilidad de exploración inicial (epsilon-greedy)\n",
        "EPSILON_DECAY = 0.999  # Factor de decaimiento de epsilon: reduce la exploración con el tiempo\n",
        "MIN_EPSILON = 0.01  # Valor mínimo de epsilon: asegura una mínima exploración continua"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5cxvI6G-oq9"
      },
      "outputs": [],
      "source": [
        "# Inicializar el agente\n",
        "agent = QLearning(\n",
        "    env=tcenv,\n",
        "    alpha=ALPHA,\n",
        "    gamma=GAMMA,\n",
        "    epsilon=EPSILON,\n",
        "    epsilon_decay=EPSILON_DECAY,\n",
        "    min_epsilon=MIN_EPSILON\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8wsy8Qnzqy1"
      },
      "outputs": [],
      "source": [
        "# --- Bucle principal de entrenamiento con QLearning ---\n",
        "rewards_per_episode = []  # Lista para almacenar la recompensa total de cada episodio\n",
        "episodes_sizes = []  # Lista para almacenar el número de pasos de cada episodio\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Resetear el entorno al inicio de cada episodio\n",
        "    obs, info = tcenv.reset(seed=semilla)\n",
        "    # Obtener las características activas del estado inicial\n",
        "    active_features = tcenv.last_active_features\n",
        "\n",
        "    total_reward = 0  # Inicializar la recompensa total del episodio\n",
        "    episode_steps = 0  # Inicializar el contador de pasos del episodio\n",
        "    done = False  # Inicializar el indicador de episodio terminado\n",
        "\n",
        "    # Bucle principal del episodio (hasta que termine o se trunque)\n",
        "    while not done:\n",
        "        # Seleccionar una acción usando la política epsilon-greedy\n",
        "        a = agent.choose_action(active_features)\n",
        "        # Ejecutar la acción en el entorno y obtener la siguiente observación, recompensa, etc.\n",
        "        obs_next, reward, done, truncated, info = tcenv.step(a)\n",
        "        # Obtener las características activas del siguiente estado\n",
        "        active_features_next = tcenv.last_active_features\n",
        "\n",
        "        # Si el episodio ha terminado o se ha truncado, salir del bucle\n",
        "        if done or truncated:\n",
        "            break\n",
        "        else:\n",
        "            # Actualizar la tabla Q con la experiencia obtenida\n",
        "            agent.update(active_features, active_features_next, a, reward)\n",
        "\n",
        "        # Actualizar el estado actual y el contador de pasos\n",
        "        active_features = active_features_next\n",
        "        episode_steps += 1\n",
        "        total_reward += reward  # Acumular la recompensa del paso\n",
        "\n",
        "    # Reducir epsilon después de cada episodio para disminuir la exploración\n",
        "    agent.decay_epsilon()\n",
        "    # Almacenar la recompensa total y el número de pasos del episodio\n",
        "    rewards_per_episode.append(total_reward)\n",
        "    episodes_sizes.append(episode_steps)\n",
        "    # Imprimir información del episodio\n",
        "    print(f\"Episode {episode+1}/{num_episodes}, total reward: {total_reward}\")\n",
        "\n",
        "# Calcular la recompensa promedio después del entrenamiento\n",
        "avg_return = np.mean(rewards_per_episode)\n",
        "print(f\"Average return over {num_episodes} episodes: {avg_return}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KPdn2HA-Zow"
      },
      "outputs": [],
      "source": [
        "plot_learning_analysis(rewards_per_episode, episodes_sizes) #  Visualiza el análisis del aprendizaje (recompensas, tamaño de episodios, etc.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TYP6UCZ-e-v"
      },
      "outputs": [],
      "source": [
        "# Obtener la política óptima, las acciones y los frames utilizando la función pi_star_from_Q\n",
        "pi, actions, frames = pi_star_from_Q(tcenv, agent.Q)\n",
        "\n",
        "# Renderizar el entorno para obtener la imagen final\n",
        "img = tcenv.render()\n",
        "\n",
        "# Agregar la imagen final a la lista de frames\n",
        "frames.append(img)\n",
        "\n",
        "# Imprimir la política óptima, las acciones y la representación del entorno\n",
        "print(\"Política óptima obtenida\\n\", pi, f\"\\n Acciones {actions} \\n Para el siguiente grid\\n\", tcenv.render())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCLj_0xX-ZsU"
      },
      "outputs": [],
      "source": [
        "create_gif(frames, \"QLearning.mp4\", fps=24)  # saves the GIF locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-cfsHHTdhxr"
      },
      "source": [
        "# Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "qpLPoxXeJqPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nCRaCfe-E_-"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = []\n",
        "        self.capacity = capacity\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "      batch = random.sample(self.buffer, batch_size)  # Usa random.sample en lugar de np.random.choice\n",
        "      return zip(*batch)  # Desempaca los valores correctamente\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "4HJrT0RsKFLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "  def __init__(self, env, alpha=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, buffer_size = 10000,  batch_size=64 ):\n",
        "\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.memory = Buffer(self.buffer_size)\n",
        "\n",
        "        self.n_actions = env.action_space.n\n",
        "        self.n_features = env.tile_size * env.n_tilings\n",
        "\n",
        "        # Inicializar DQN y optimizador\n",
        "        self.policy_net = NN(self.n_features, self.n_actions).to(device)\n",
        "        self.target_net = NN(self.n_features, self.n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=alpha)\n",
        "\n",
        "  def select_action(self, state):\n",
        "      if np.random.rand() < self.epsilon:\n",
        "          return np.random.randint(self.n_actions)\n",
        "\n",
        "      # Create a one-hot encoded state representation\n",
        "      state_tensor = torch.zeros(1, self.n_features, device=device, dtype=torch.float32)  # Create a zero-filled tensor\n",
        "      state_tensor[0, state] = 1  # Set the active features to 1\n",
        "\n",
        "      with torch.no_grad():\n",
        "          return self.policy_net(state_tensor).argmax().item()\n",
        "\n",
        "\n",
        "  def optimize(self, state):\n",
        "          batch = self.memory.sample(self.batch_size)\n",
        "          states, actions, rewards, next_states, dones = map(np.array, batch)\n",
        "          states_np = states # numpy array of list of active features\n",
        "          next_states_np = next_states # numpy array of list of active features\n",
        "\n",
        "          actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
        "          rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
        "          dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
        "          states = torch.zeros(self.batch_size, self.n_features, device=device, dtype=torch.float32)  # Create a zero-filled tensor\n",
        "          for i, s in enumerate(states_np):\n",
        "              states[i, s] = 1  # Set the active features to 1\n",
        "          next_states = torch.zeros(self.batch_size, self.n_features, device=device, dtype=torch.float32)  # Create a zero-filled tensor\n",
        "          for i, ns in enumerate(next_states_np):\n",
        "              next_states[i, ns] = 1  # Set the active features to 1\n",
        "\n",
        "          q_values = self.policy_net(states).gather(1, actions).squeeze()\n",
        "          next_q_values = self.target_net(next_states).max(1)[0]\n",
        "          expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "          loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "\n",
        "  def decay_epsilon(self):\n",
        "      self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n"
      ],
      "metadata": {
        "id": "dVzI7bYB_zRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99  # Factor de descuento\n",
        "alpha = 0.001  # Tasa de aprendizaje\n",
        "epsilon = 1.0  # Probabilidad inicial de exploración\n",
        "epsilon_min = 0.01  # Probabilidad mínima de exploración\n",
        "epsilon_decay = 0.995  # Decaimiento de epsilon\n",
        "buffer_size = 10000  # Tamaño del buffer de memoria\n",
        "batch_size = 64  # Tamaño del minibatch\n",
        "num_episodes = 20000  # Número total de episodios"
      ],
      "metadata": {
        "id": "fzY3uX8uODNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQN(tcenv, alpha, gamma, epsilon, epsilon_decay, epsilon_min, buffer_size, batch_size)"
      ],
      "metadata": {
        "id": "FJTz7yToCCXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Bucle principal de entrenamiento con QLearning ---\n",
        "rewards_per_episode = []  # Lista para almacenar la recompensa total de cada episodio\n",
        "episodes_sizes = []  # Lista para almacenar el número de pasos de cada episodio\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = tcenv.reset()\n",
        "    state = tcenv.last_active_features  # Obtener las *active features*\n",
        "    total_reward = 0\n",
        "    episode_steps = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, terminated, truncated, _ = tcenv.step(action)\n",
        "        next_state = tcenv.last_active_features  # Obtener las *active features* del siguiente estado\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent.memory.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        episode_steps += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        if len(agent.memory) > agent.batch_size:\n",
        "            # Entrenar DQN con una muestra del buffer\n",
        "            agent.optimize(state)\n",
        "\n",
        "\n",
        "    # Reducir epsilon\n",
        "    agent.decay_epsilon()\n",
        "\n",
        "    # Almacenar la recompensa total y el número de pasos del episodio\n",
        "    rewards_per_episode.append(total_reward)\n",
        "    episodes_sizes.append(episode_steps)\n",
        "\n",
        "\n",
        "    # Actualizar la red objetivo periódicamente\n",
        "    if episode % 10 == 0:\n",
        "        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode+1}/{num_episodes}, total reward: {total_reward}\")\n",
        "\n",
        "# Calcular la recompensa promedio después del entrenamiento\n",
        "avg_return = np.mean(rewards_per_episode)\n",
        "print(f\"Average return over {num_episodes} episodes: {avg_return}\")\n"
      ],
      "metadata": {
        "id": "vBjBTbbnKVy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wUjparq6FQC"
      },
      "outputs": [],
      "source": [
        "plot_learning_analysis(rewards_per_episode, episodes_sizes) #  Visualiza el análisis del aprendizaje (recompensas, tamaño de episodios, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQyRmBDrPat0"
      },
      "outputs": [],
      "source": [
        "def pi_star_from_Deep(env, agent):\n",
        "    frames = []\n",
        "    done = False\n",
        "    pi_star = {}\n",
        "\n",
        "    obs, info = env.reset(seed=semilla)\n",
        "    active_features = env.last_active_features  # Obtener las active features iniciales\n",
        "    img = env.render()\n",
        "    frames.append(img)\n",
        "\n",
        "    actions = \"\"\n",
        "    agent.policy_net.eval()\n",
        "\n",
        "    while not done:\n",
        "        # Crear tensor de estado con las active features\n",
        "        state_tensor = torch.zeros(1, agent.n_features, device=device, dtype=torch.float32)\n",
        "        state_tensor[0, active_features] = 1  # Establecer las active features en 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = agent.policy_net(state_tensor).argmax().item()\n",
        "\n",
        "        # Convert obs to a tuple if it's not already\n",
        "        obs_tuple = tuple(obs) if isinstance(obs, (list, np.ndarray)) else obs\n",
        "        pi_star[obs_tuple] = action\n",
        "        actions += f\"{action}, \"\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        img = env.render()\n",
        "        frames.append(img)\n",
        "\n",
        "        done = terminated or truncated\n",
        "        active_features = env.last_active_features  # Actualizar active features\n",
        "\n",
        "    return pi_star, actions, frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5crKdl86FTH"
      },
      "outputs": [],
      "source": [
        "# Obtener la política óptima, las acciones y los frames utilizando la función pi_star_from_Q\n",
        "pi, actions, frames = pi_star_from_Deep(tcenv, agent)\n",
        "\n",
        "# Renderizar el entorno para obtener la imagen final\n",
        "img = tcenv.render()\n",
        "\n",
        "# Agregar la imagen final a la lista de frames\n",
        "frames.append(img)\n",
        "\n",
        "# Imprimir la política óptima, las acciones y la representación del entorno\n",
        "print(\"Política óptima obtenida\\n\", pi, f\"\\n Acciones {actions} \\n Para el siguiente grid\\n\", tcenv.render())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUwFJ3wK6Feo"
      },
      "outputs": [],
      "source": [
        "create_gif(frames, \"DeepQLearning.mp4\", fps=24)  # saves the GIF locally"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}