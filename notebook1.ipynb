{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al problema\n",
    "\n",
    "En la práctica anterior desarrollamos el problema del bandido de k-brazos, lo cual sirvió de manera introductoria para empezar a conocer y manejar conceptos de aprendizaje por refuerzo. Sin embargo, ese problema está libre de contexto, lo cual en problemas cotidianos no suele ser así. Por ello en esta práctica se usará Gimnasium para resolver diferentes problemas de aprendizaje por refuerzo propuestos por la librería. Para ello se implementarán una diversidad de algoritmos que parten de las ecuaciones de Bellman. Por lo que el objetivo de esta práctica es el familiarizarse con Gimnasium y poder implementar algoritmos de aprendizaje por refuerzo en diferentes problemas que sean capaces de resolver el problema de la toma de decisiones en un entorno complejo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento\n",
    "Se llevan a cabo los siguientes experimentos:\n",
    "- Frozen Lake\n",
    "- Taxi\n",
    "- Mountain Car\n",
    "- Lunar Lander\n",
    "\n",
    "Los entornos han sido elegidos para poder demostrar la utilidad de los algoritmos de aprendizaje por refuerzo en estos problemas. Así como otras técnicas como el tiling. Con esto se pretende observar y apreciar diferentes algoritmos, sus diferencias, cuando son más útiles, si resuelven el problema o no, y si se pueden mejorar. Además para cada estudio se incluirán diversos agentes con sus entrenamientos; así como el análisis de sus resultados, que incluirá mostrar gráficamente la evolución de las recompensas y tamaños de los episodios. Así como una función de evaluación de la solución usando la política final del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "El problema de **Frozen Lake** es un entorno clásico de aprendizaje por refuerzo proporcionado, diseñado para estudiar la toma de decisiones en un entorno estocástico. En este escenario, un agente debe navegar por un lago congelado representado como una cuadrícula, desde una posición inicial hasta un objetivo, evitando caer en agujeros. Este entorno es ideal para probar algoritmos clasicos de aprendizaje por refuerzo, ya que pone a prueba la capacidad del agente para equilibrar **exploración y explotación** en un ambiente donde las transiciones no son siempre predecibles. Este problema, además es simple de resolver, tiene un espacio de observación pequeño y discreto, por lo que servirá de manera introductoria para aprender los algoritmos de aprendizaje por refuerzo y familiarizarnos con Gimnasium. Además veremos la utilidad de diferentes algoritmos para este problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi\n",
    "El problema de **Taxi** es un entorno clásico de aprendizaje por refuerzo, en donde un agente debe recoger y dejar pasajeros en ubicaciones específicas dentro de una cuadrícula. El entorno presenta una dinámica determinista: el taxi puede moverse en cuatro direcciones (arriba, abajo, izquierda, derecha) y ejecutar acciones de recoger o dejar pasajeros. Sin embargo, el desafío radica en encontrar la mejor estrategia para minimizar el número de pasos y maximizar la recompensa, ya que las acciones incorrectas, como intentar recoger o dejar pasajeros en ubicaciones incorrectas, generan penalizaciones. Este problema es ideal para probar algoritmos clásicos; porque permite estudiar cómo un agente aprende a tomar decisiones óptimas en un entorno discreto con recompensas bien definidas. Además a comparación del Frozen Lake, en este problema el espacio de observación es más grande, por lo que se vuelve interesante el ver el funcionamiento de los algoritmos de aprendizaje por refuerzo en el problema y compararlos con el problema anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car\n",
    "El problema de **Mountain Car** es un entorno clásico de aprendizaje por refuerzo, donde un coche debe aprender a escalar una colina para llegar a la cima. Sin embargo, el coche no tiene suficiente potencia para alcanzar la cima directamente, por lo que debe aprender a marchar atrás para coger fuerza con la anterior colina. El agente puede acelerar hacia la izquierda, la derecha o no acelerar, y recibe una recompensa negativa en cada paso hasta que alcance la meta. Este problema es un ejemplo de control continuo, ya que su espacio de estados es continuo y su acción es discreta. Este problema será útil para ver la utilidad de los Tiling y el cómo discretizar el espacio de estados, por lo que nos permitirá valorar esto y el cómo se comportan una gran variedad de algoritmos de aprendizaje por refuerzo ante esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander\n",
    "El problema de **Lunar Lander** es un entorno de aprendizaje, donde un agente debe aprender a controlar un módulo de aterrizaje lunar para descender de manera segura en una plataforma designada. El agente puede activar motores para moverse a la izquierda, derecha o frenar su caída, y recibe recompensas en función de su estabilidad, proximidad a la plataforma y aterrizaje exitoso. Sin embargo, perder el control o aterrizar fuera de la zona segura genera penalizaciones. Este problema es desafiante porque requiere equilibrio entre consumo de combustible, estabilidad y precisión. Este problema es el más desafiante de todos los problemas de la práctica 2 y el más complejo computacionalmente. Por lo que veremos las limitaciones de los algoritmos de aprendizaje en entornos complejos. También veremos las limitaciones de tiling y pondremos de manifiesto la utilidad de las técnicas de aproximación, sobre todo las no lineales, dado la alta complejidad del problema."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
